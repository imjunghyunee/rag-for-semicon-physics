import streamlit as st
import sys
import os
from pathlib import Path
import time
import json
from typing import Dict, Any, List, Tuple
import asyncio
import traceback
import tempfile
import shutil
import uuid
from datetime import datetime
import concurrent.futures

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from rag_pipeline.graph_builder import build_graph
from rag_pipeline.graph_state import GraphState
from rag_pipeline import config, utils
from rag_pipeline.plan_execute_langgraph import PlanExecuteLangGraph

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Semiconductor Physics RAG System",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .step-container {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
        background-color: #f8f9fa;
    }
    .step-header {
        font-size: 1.3rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .plan-step {
        background-color: #e3f2fd;
        padding: 0.5rem;
        margin: 0.3rem 0;
        border-left: 4px solid #2196f3;
        border-radius: 4px;
    }
    .context-box {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        max-height: 300px;
        overflow-y: auto;
    }
    .result-box {
        background-color: #e8f5e8;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
    .complexity-simple {
        color: #28a745;
        font-weight: bold;
    }
    .complexity-complex {
        color: #dc3545;
        font-weight: bold;
    }
    .loading-spinner {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid #f3f3f3;
        border-top: 3px solid #3498db;
        border-radius: 50%;
        animation: spin 1s linear infinite;
    }
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
</style>
""", unsafe_allow_html=True)


def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_step" not in st.session_state:
        st.session_state.current_step = 0
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "complexity" not in st.session_state:
        st.session_state.complexity = None
    if "plan_results" not in st.session_state:
        st.session_state.plan_results = []
    if "final_answer" not in st.session_state:
        st.session_state.final_answer = None
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = {"pdf": None, "images": []}
    if "temp_dir" not in st.session_state:
        st.session_state.temp_dir = None


def save_uploaded_files(uploaded_files, file_type="image"):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    if not uploaded_files:
        return None
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    if st.session_state.temp_dir is None:
        st.session_state.temp_dir = tempfile.mkdtemp(prefix="streamlit_rag_")
    
    temp_dir = Path(st.session_state.temp_dir)
    
    if file_type == "pdf":
        # PDF íŒŒì¼ í•˜ë‚˜ë§Œ ì²˜ë¦¬
        pdf_file = uploaded_files
        pdf_path = temp_dir / pdf_file.name
        with open(pdf_path, "wb") as f:
            f.write(pdf_file.getbuffer())
        return pdf_path
    
    elif file_type == "image":
        # ì—¬ëŸ¬ ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
        if len(uploaded_files) == 1:
            # ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼
            image_file = uploaded_files[0]
            image_path = temp_dir / image_file.name
            with open(image_path, "wb") as f:
                f.write(image_file.getbuffer())
            return image_path
        else:
            # ì—¬ëŸ¬ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ ë””ë ‰í† ë¦¬ì— ì €ì¥
            images_dir = temp_dir / "images"
            images_dir.mkdir(exist_ok=True)
            
            for i, image_file in enumerate(uploaded_files):
                image_path = images_dir / f"{i:03d}_{image_file.name}"
                with open(image_path, "wb") as f:
                    f.write(image_file.getbuffer())
            
            return images_dir
    
    return None


def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
    if st.session_state.temp_dir and Path(st.session_state.temp_dir).exists():
        try:
            shutil.rmtree(st.session_state.temp_dir)
            st.session_state.temp_dir = None
            print("Temporary files cleaned up")
        except Exception as e:
            print(f"Error cleaning up temp files: {e}")


def display_file_upload_section():
    """íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ í‘œì‹œ"""
    st.markdown("### ğŸ“ File Upload (Optional)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“„ PDF Document**")
        uploaded_pdf = st.file_uploader(
            "Upload a PDF file for analysis",
            type=['pdf'],
            key="pdf_uploader",
            help="Upload a PDF document to extract text and analyze with your question"
        )
        
        if uploaded_pdf:
            st.success(f"âœ… PDF uploaded: {uploaded_pdf.name}")
            st.session_state.uploaded_files["pdf"] = uploaded_pdf
        else:
            st.session_state.uploaded_files["pdf"] = None
    
    with col2:
        st.markdown("**ğŸ–¼ï¸ Images**")
        uploaded_images = st.file_uploader(
            "Upload image(s) for analysis",
            type=['png', 'jpg', 'jpeg', 'bmp', 'tiff'],
            accept_multiple_files=True,
            key="image_uploader",
            help="Upload one or more images to extract text and analyze with your question"
        )
        
        if uploaded_images:
            st.success(f"âœ… {len(uploaded_images)} image(s) uploaded")
            for img in uploaded_images:
                st.caption(f"ğŸ“¸ {img.name}")
            st.session_state.uploaded_files["images"] = uploaded_images
        else:
            st.session_state.uploaded_files["images"] = []
    
    # íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
    if st.session_state.uploaded_files["pdf"] or st.session_state.uploaded_files["images"]:
        st.info("ğŸ’¡ Files will be processed together with your question when submitted.")
        
        # íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì˜µì…˜
        with st.expander("ğŸ” File Preview", expanded=False):
            if st.session_state.uploaded_files["pdf"]:
                st.markdown("**PDF File:**")
                st.text(f"ğŸ“„ {st.session_state.uploaded_files['pdf'].name}")
                st.text(f"ğŸ“ Size: {st.session_state.uploaded_files['pdf'].size:,} bytes")
            
            if st.session_state.uploaded_files["images"]:
                st.markdown("**Image Files:**")
                for i, img in enumerate(st.session_state.uploaded_files["images"], 1):
                    col_img1, col_img2 = st.columns([3, 1])
                    with col_img1:
                        st.image(img, caption=f"{i}. {img.name}", width=200)
                    with col_img2:
                        st.text(f"ğŸ“ Size: {img.size:,} bytes")


async def process_query_with_files(query: str, pdf_file=None, image_files=None):
    """íŒŒì¼ê³¼ í•¨ê»˜ ì¿¼ë¦¬ ì²˜ë¦¬ - ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬í•˜ê³  ì¼ë°˜ì ì¸ íë¦„ìœ¼ë¡œ ê²°ê³¼ í‘œì‹œ"""
    
    # íŒŒì¼ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬)
    pdf_path = None
    img_path = None
    
    try:
        if pdf_file:
            pdf_path = save_uploaded_files(pdf_file, "pdf")
            print(f"PDF saved to: {pdf_path}")
        
        if image_files:
            img_path = save_uploaded_files(image_files, "image")
            print(f"Images saved to: {img_path}")
        
        # ê·¸ë˜í”„ ë¹Œë“œ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬)
        graph = build_graph(
            pdf_path=pdf_path,
            img_path=img_path,
            retrieval_type=config.RETRIEVAL_TYPE,
            hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
        )
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        init_state: GraphState = {"question": [query], "messages": [("user", query)]}
        
        # LangGraph ì‹¤í–‰ - ì¼ë°˜ì ì¸ ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ê³¼ ë™ì¼í•˜ê²Œ ì²˜ë¦¬
        async for event in graph.astream(init_state):
            for node_name, node_output in event.items():
                if node_name == "__end__":
                    continue
                    
                # ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ (íŒŒì¼ ì²˜ë¦¬ ì—¬ë¶€ì™€ ê´€ê³„ì—†ì´ ë™ì¼)
                await display_node_execution(node_name, node_output, query)
                    
                # ì ì‹œ ëŒ€ê¸° (ì‚¬ìš©ìê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
                await asyncio.sleep(1)
        
        # ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        final_state = graph.invoke(init_state)
        
        # ìµœì¢… ê²°ê³¼ í‘œì‹œ (ì¼ë°˜ì ì¸ ë°©ì‹ê³¼ ë™ì¼)
        display_final_results(final_state, query)
            
        return final_state
                
    except Exception as e:
        st.error(f"âŒ Error processing query with files: {str(e)}")
        print(f"âŒ Full error traceback:")
        print(traceback.format_exc())
        return None


def display_complexity_result(complexity: str):
    """ë³µì¡ë„ íŒì • ê²°ê³¼ í‘œì‹œ"""
    if complexity == "simple":
        st.markdown(
            f'<div class="complexity-simple">ğŸ“ Question Complexity: SIMPLE</div>',
            unsafe_allow_html=True
        )
        st.info("This question will be processed using direct retrieval.")
    else:
        st.markdown(
            f'<div class="complexity-complex">ğŸ§  Question Complexity: COMPLEX</div>',
            unsafe_allow_html=True
        )
        st.info("This question requires multi-step reasoning and will be processed using Plan-and-Execute.")


def display_simple_processing(query: str):
    """Simple ì§ˆë¬¸ ì²˜ë¦¬ ê³¼ì • í‘œì‹œ"""
    st.markdown("### ğŸ” Processing Simple Query")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ê·¸ë˜í”„ êµ¬ì„±
        status_text.text("Building retrieval pipeline...")
        progress_bar.progress(20)
        
        graph = build_graph(
            pdf_path=None,
            img_path=None,
            retrieval_type=config.RETRIEVAL_TYPE,
            hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
        )
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        status_text.text("Processing query...")
        progress_bar.progress(50)
        
        init_state: GraphState = {"question": [query], "messages": [("user", query)]}
        final_state = graph.invoke(init_state)
        
        progress_bar.progress(80)
        status_text.text("Generating final answer...")
        
        # ê²°ê³¼ í‘œì‹œ
        progress_bar.progress(100)
        status_text.text("Complete!")
        time.sleep(0.5)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        # ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
        if "context" in final_state and final_state["context"]:
            with st.expander("ğŸ“š Retrieved Context", expanded=False):
                contexts = final_state["context"]
                for i, doc in enumerate(contexts[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                    st.markdown(f"**Context {i}:**")
                    st.markdown(f'<div class="context-box">{content[:500]}...</div>', 
                              unsafe_allow_html=True)
        
        # ìµœì¢… ë‹µë³€ í‘œì‹œ
        st.markdown("### ğŸ’¡ Final Answer")
        answer = final_state.get("answer", "No answer generated")
        st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
        
        return final_state
        
    except Exception as e:
        st.error(f"Error processing simple query: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def process_complex_query_step_by_step(query: str):
    """Complex ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
    
    # Plan-and-Execute ì—ì´ì „íŠ¸ ìƒì„±
    agent = PlanExecuteLangGraph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "input": query,
        "plan": [],
        "past_steps": [],
        "response": "",
        "current_step_index": 0,
        "retrieval_type": config.RETRIEVAL_TYPE,
        "hybrid_weights": [config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT],
        "all_context_docs": [],
        "combined_context": ""
    }
    
    # í”„ë¡œê·¸ë ˆìŠ¤ ì¶”ì ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
    step_container = st.empty()
    
    try:
        # Step 1: Planning
        with step_container.container():
            st.markdown("### ğŸ“‹ Step 1: Creating Execution Plan")
            with st.spinner("Planning execution steps..."):
                plan_result = await agent.plan_step(initial_state)
                initial_state.update(plan_result)
            
            # ê³„íš í‘œì‹œ
            if initial_state["plan"]:
                st.markdown("**Generated Plan:**")
                for i, step in enumerate(initial_state["plan"], 1):
                    st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                              unsafe_allow_html=True)
            
            time.sleep(2)  # ì‚¬ìš©ìê°€ ê³„íšì„ ì½ì„ ì‹œê°„
        
        # Step 2+: Execution Loop
        step_num = 2
        max_iterations = config.MAX_PLAN_STEPS + 2  # ì•ˆì „ ì¥ì¹˜
        
        while step_num <= max_iterations:
            current_step_index = initial_state.get("current_step_index", 0)
            plan = initial_state.get("plan", [])
            
            # ì‹¤í–‰í•  ë‹¨ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
            if current_step_index >= len(plan):
                break
                
            # í˜„ì¬ ë‹¨ê³„ í‘œì‹œ
            with step_container.container():
                current_step = plan[current_step_index]
                st.markdown(f"### ğŸ”„ Step {step_num}: Executing Plan")
                st.markdown(f"**Current Step:** {current_step}")
                
                with st.spinner(f"Executing: {current_step[:50]}..."):
                    # ë‹¨ê³„ ì‹¤í–‰
                    execute_result = await agent.execute_step(initial_state)
                    initial_state.update(execute_result)
                
                # ì‹¤í–‰ ê²°ê³¼ í‘œì‹œ
                past_steps = initial_state.get("past_steps", [])
                if past_steps:
                    latest_step, latest_result = past_steps[-1]
                    
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.markdown("**Retrieved Context:**")
                        combined_context = initial_state.get("combined_context", "")
                        if combined_context:
                            # ê°€ì¥ ìµœê·¼ ë‹¨ê³„ì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ
                            recent_context = combined_context.split("=== Step")[-1]
                            st.markdown(f'<div class="context-box">{recent_context[:400]}...</div>', 
                                      unsafe_allow_html=True)
                    
                    with col2:
                        st.markdown("**Step Result:**")
                        st.markdown(f'<div class="result-box">{latest_result[:300]}...</div>', 
                                  unsafe_allow_html=True)
                
                time.sleep(1.5)  # ê²°ê³¼ë¥¼ ì½ì„ ì‹œê°„
            
            # Replan ë‹¨ê³„
            step_num += 1
            with step_container.container():
                st.markdown(f"### ğŸ¤” Step {step_num}: Evaluating Progress")
                
                with st.spinner("Evaluating progress and replanning..."):
                    replan_result = await agent.replan_step(initial_state)
                    initial_state.update(replan_result)
                
                # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
                if "response" in replan_result and replan_result["response"]:
                    # ìµœì¢… ë‹µë³€ ìƒì„±ë¨
                    break
                elif "plan" in replan_result:
                    # ìƒˆë¡œìš´ ê³„íš ìƒì„±ë¨
                    st.markdown("**Updated Plan:**")
                    new_plan = replan_result["plan"]
                    for i, step in enumerate(new_plan, 1):
                        st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                                  unsafe_allow_html=True)
                
                time.sleep(1)
            
            step_num += 1
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_num > max_iterations:
                st.warning("Maximum iteration limit reached.")
                break
        
        # ìµœì¢… ë‹µë³€ í‘œì‹œ
        step_container.empty()  # ì´ì „ ë‹¨ê³„ë“¤ ì •ë¦¬
        
        # ê³¼ì • ìš”ì•½ì„ expanderë¡œ í‘œì‹œ
        with st.expander("ğŸ“Š Process Summary", expanded=False):
            st.markdown("**Original Plan:**")
            for i, step in enumerate(initial_state.get("plan", []), 1):
                st.markdown(f"{i}. {step}")
            
            st.markdown("**Executed Steps:**")
            for i, (step, result) in enumerate(initial_state.get("past_steps", []), 1):
                st.markdown(f"**Step {i}:** {step}")
                st.markdown(f"*Result:* {result[:200]}...")
        
        # ìµœì¢… ë‹µë³€
        st.markdown("### ğŸ’¡ Final Answer")
        final_response = initial_state.get("response", "")
        if not final_response:
            # responseê°€ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±
            final_response = await agent._generate_final_response(initial_state)
        
        st.markdown(f'<div class="result-box">{final_response}</div>', 
                  unsafe_allow_html=True)
        
        return initial_state
        
    except Exception as e:
        st.error(f"Error in complex query processing: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def process_query_with_langgraph_streaming(query: str):
    """LangGraphì˜ ì‹¤í–‰ íë¦„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ì—¬ í‘œì‹œ"""
    
    # LangGraph ë¹Œë“œ
    graph = build_graph(
        pdf_path=None,
        img_path=None,
        retrieval_type=config.RETRIEVAL_TYPE,
        hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
    )
    
    # ì´ˆê¸° ìƒíƒœ
    init_state: GraphState = {"question": [query], "messages": [("user", query)]}
    
    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
    main_container = st.empty()
    progress_container = st.empty()
    
    try:
        # LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        async for event in graph.astream(init_state):
            for node_name, node_output in event.items():
                if node_name == "__end__":
                    continue
                    
                # ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
                with main_container.container():
                    await display_node_execution(node_name, node_output, query)
                    
                # ì ì‹œ ëŒ€ê¸° (ì‚¬ìš©ìê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
                await asyncio.sleep(1)
        
        # ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        final_state = graph.invoke(init_state)
        
        # ìµœì¢… ê²°ê³¼ í‘œì‹œ
        with main_container.container():
            display_final_results(final_state, query)
            
        return final_state
        
    except Exception as e:
        st.error(f"Error in LangGraph execution: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def display_node_execution(node_name: str, node_output: Dict[str, Any], query: str):
    """ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‘œì‹œ"""
    
    if node_name == "complexity_check":
        # ë³µì¡ë„ ì²´í¬ ê²°ê³¼ í‘œì‹œ
        complexity = node_output.get("next", "unknown")
        st.markdown("### ğŸ” Step 1: Analyzing Question Complexity")
        display_complexity_result(complexity)
        
        if complexity == "simple":
            st.info("ğŸ”„ Proceeding with simple retrieval pipeline...")
        else:
            st.info("ğŸ§  Proceeding with complex Plan-and-Execute pipeline...")
            
    elif node_name in ["retrieve_simple"]:
        # Simple ê²€ìƒ‰ ë‹¨ê³„
        st.markdown("### ğŸ“š Step 2: Information Retrieval")
        with st.spinner("Searching relevant documents..."):
            await asyncio.sleep(0.5)  # ì‹œê°ì  íš¨ê³¼
        
        context = node_output.get("context", [])
        st.success(f"âœ… Retrieved {len(context)} relevant documents")
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
        if context:
            with st.expander("ğŸ“– Preview Retrieved Documents", expanded=False):
                for i, doc in enumerate(context[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ
                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                    st.markdown(f"**Document {i}:**")
                    st.markdown(f'<div class="context-box">{content[:300]}...</div>', 
                              unsafe_allow_html=True)
    
    elif node_name == "relevance_check":
        # ê´€ë ¨ì„± ì²´í¬ ë‹¨ê³„
        st.markdown("### ğŸ¯ Step 3: Relevance Filtering")
        with st.spinner("Filtering relevant content..."):
            await asyncio.sleep(0.5)
        
        filtered_context = node_output.get("filtered_context", [])
        scores = node_output.get("scores", [])
        
        if scores:
            avg_score = sum(scores) / len(scores)
            st.success(f"âœ… Filtered content (avg similarity: {avg_score:.3f})")
        else:
            st.success("âœ… Content filtering completed")
    
    elif node_name == "llm_answer_simple":
        # Simple ë‹µë³€ ìƒì„±
        st.markdown("### ğŸ’¡ Step 4: Generating Answer")
        with st.spinner("Generating final answer..."):
            await asyncio.sleep(1)
        
        answer = node_output.get("answer", "")
        if answer:
            st.success("âœ… Answer generated successfully")
        
    elif node_name == "plan_and_execute":
        # Complex ì²˜ë¦¬ ê²°ê³¼
        st.markdown("### ğŸ§  Complex Query Processing Results")
        
        # Plan í‘œì‹œ
        plan = node_output.get("plan", [])
        if plan:
            st.markdown("**ğŸ“‹ Execution Plan:**")
            for i, step in enumerate(plan, 1):
                st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                          unsafe_allow_html=True)
        
        # ì‹¤í–‰ëœ ë‹¨ê³„ë“¤ í‘œì‹œ
        executed_steps = node_output.get("executed_steps", [])
        if executed_steps:
            with st.expander(f"ğŸ“Š Executed Steps ({len(executed_steps)} steps)", expanded=False):
                for i, (step, result) in enumerate(executed_steps, 1):
                    st.markdown(f"**Step {i}:** {step}")
                    st.markdown(f"*Result:* {result[:200]}...")
                    st.markdown("---")
        
        # ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
        combined_context = node_output.get("combined_context", "")
        if combined_context:
            with st.expander("ğŸ“š Retrieved Context", expanded=False):
                st.markdown(f'<div class="context-box">{combined_context[:800]}...</div>', 
                          unsafe_allow_html=True)


def save_graph_state_to_output(final_state: Dict[str, Any], query: str) -> str:
    """ìµœì¢… GraphStateë¥¼ output ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # GraphState í˜•ì‹ì— ë§ì¶° ë°ì´í„° êµ¬ì„±
        graph_state_dict = {
            "question": [query],
            "explanation": final_state.get("explanation", ""),
            "context": final_state.get("context", []),
            "filtered_context": final_state.get("filtered_context", []),
            "examples": final_state.get("examples", ""),
            "answer": final_state.get("answer", ""),
            "messages": final_state.get("messages", []),
            "scores": final_state.get("scores", []),
            "filtered_scores": final_state.get("filtered_scores", []),
            "subquestions": final_state.get("subquestions", []),
            "subquestion_results": final_state.get("subquestion_results", []),
            "combined_context": final_state.get("combined_context", ""),
            "plan": final_state.get("plan", []),
            "executed_steps": final_state.get("executed_steps", []),
            "retrieval_type": config.RETRIEVAL_TYPE,
            "hybrid_weights": [config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT],
            "next": final_state.get("next", "")
        }
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + UUID)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        filename = f"graph_state_{timestamp}_{unique_id}.json"
        file_path = output_dir / filename
        
        # JSONìœ¼ë¡œ ì €ì¥ (ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜)
        serializable_state = convert_to_serializable(graph_state_dict)
        
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(serializable_state, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Graph state saved to: {file_path}")
        return str(file_path)
        
    except Exception as e:
        print(f"âŒ Error saving graph state: {e}")
        return ""


def convert_to_serializable(obj):
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if hasattr(obj, 'page_content'):  # Document ê°ì²´
        return {
            "page_content": obj.page_content,
            "metadata": getattr(obj, 'metadata', {})
        }
    elif hasattr(obj, 'content'):  # Message ê°ì²´
        return obj.content
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, tuple):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj


def save_graph_visualizations(final_state: Dict[str, Any], query: str) -> List[str]:
    """ë‘ ê°€ì§€ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥"""
    saved_files = []
    
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Main Graph (graph_builder.pyì—ì„œ ìƒì„±ëœ ê·¸ë˜í”„) ì‹œê°í™”
        try:
            print("ğŸ“Š Generating main graph visualization...")
            
            # ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ê·¸ë˜í”„ ì¬ìƒì„±
            pdf_file = st.session_state.uploaded_files.get("pdf")
            image_files = st.session_state.uploaded_files.get("images", [])
            
            pdf_path = None
            img_path = None
            if pdf_file:
                pdf_path = save_uploaded_files(pdf_file, "pdf")
            if image_files:
                img_path = save_uploaded_files(image_files, "image")
            
            main_graph = build_graph(
                pdf_path=pdf_path,
                img_path=img_path,
                retrieval_type=config.RETRIEVAL_TYPE,
                hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
            )
            
            main_graph_path = output_dir / f"main_graph_{timestamp}.png"
            main_graph.get_graph().draw_mermaid_png(output_file_path=str(main_graph_path))
            
            saved_files.append(str(main_graph_path))
            print(f"   âœ… Main graph saved to: {main_graph_path}")
            
        except Exception as e:
            print(f"   âŒ Error saving main graph: {e}")
        
        # 2. Plan-Execute Graph ì‹œê°í™” (ë³µì¡í•œ ì§ˆë¬¸ì¸ ê²½ìš°ë§Œ)
        if final_state.get("executed_steps") or final_state.get("plan"):
            try:
                print("ğŸ“Š Generating plan-execute graph visualization...")
                
                from rag_pipeline.plan_execute_langgraph import PlanExecuteLangGraph
                plan_execute_agent = PlanExecuteLangGraph()
                
                plan_execute_graph_path = output_dir / f"plan_execute_graph_{timestamp}.png"
                plan_execute_agent.visualize_graph(output_path=plan_execute_graph_path)
                
                saved_files.append(str(plan_execute_graph_path))
                print(f"   âœ… Plan-execute graph saved to: {plan_execute_graph_path}")
                
            except Exception as e:
                print(f"   âŒ Error saving plan-execute graph: {e}")
        
        return saved_files
        
    except Exception as e:
        print(f"âŒ Error in graph visualization: {e}")
        return []


def display_final_results(final_state: Dict[str, Any], query: str):
    """ìµœì¢… ê²°ê³¼ í‘œì‹œ - ì €ì¥ ê¸°ëŠ¥ì„ ë°±ê·¸ë¼ìš´ë“œë¡œ ì´ë™"""
    st.markdown("---")
    st.markdown("### ğŸ‰ Final Results")
    
    # ìµœì¢… ë‹µë³€
    answer = final_state.get("answer", "No answer generated")
    st.markdown("#### ğŸ’¡ Answer:")
    st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
    
    # ì²˜ë¦¬ ê²½ë¡œ ìš”ì•½
    explanation = final_state.get("explanation", "")
    if explanation:
        st.markdown("#### ğŸ“ Processing Summary:")
        st.info(explanation)
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        context_count = len(final_state.get("context", []))
        st.metric("ğŸ“š Documents Retrieved", context_count)
    
    with col2:
        # Plan-Execute ì •ë³´
        executed_steps = final_state.get("executed_steps", [])
        if executed_steps:
            st.metric("ğŸ”„ Steps Executed", len(executed_steps))
        else:
            st.metric("ğŸ” Processing Type", "Simple Retrieval")
    
    with col3:
        scores = final_state.get("scores", [])
        if scores and len(scores) > 0:
            avg_score = sum(scores) / len(scores)
            st.metric("ğŸ¯ Avg Similarity", f"{avg_score:.3f}")
    
    # ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ìë™ ì €ì¥ (UI ì—†ì´)
    try:
        # Graph State ì €ì¥
        saved_state_file = save_graph_state_to_output(final_state, query)
        if saved_state_file:
            print(f"âœ… Graph state auto-saved to: {saved_state_file}")
        
        # Graph ì‹œê°í™” ì €ì¥
        saved_graph_files = save_graph_visualizations(final_state, query)
        if saved_graph_files:
            print(f"âœ… Graph visualizations auto-saved:")
            for graph_file in saved_graph_files:
                print(f"   ğŸ“ˆ {Path(graph_file).name}")
        
    except Exception as e:
        print(f"âŒ Error in background auto-save: {e}")


def main():
    """ë©”ì¸ Streamlit ì•±"""
    initialize_session_state()
    
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ”¬ Semiconductor Physics RAG System</div>', 
              unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("### âš™ï¸ Configuration")
        st.markdown(f"**Model:** {config.OPENAI_MODEL}")
        st.markdown(f"**Retrieval Type:** {config.RETRIEVAL_TYPE}")
        st.markdown(f"**TOP_K:** {config.TOP_K}")
        st.markdown(f"**Max Plan Steps:** {config.MAX_PLAN_STEPS}")
        
        st.markdown("### ğŸ“Š Graph Flow")
        st.markdown("""
        **LangGraph Execution:**
        1. ğŸ” Complexity Check
        2a. ğŸ“š Simple: Retrieve â†’ Filter â†’ Answer
        2b. ğŸ§  Complex: Plan-and-Execute
        3. ğŸ’¡ Final Answer
        """)
        
        st.markdown("### ğŸ“ File Processing")
        st.markdown("""
        **Supported Files:**
        - ğŸ“„ PDF documents
        - ğŸ–¼ï¸ Images (PNG, JPG, etc.)
        
        **Processing Flow:**
        1. File Upload
        2. Text Extraction (OCR/PDF parsing)
        3. Content Integration
        4. Query Processing
        """)
        
        # ğŸ”¥ ìë™ ì €ì¥ ì •ë³´ ì¶”ê°€
        st.markdown("### ğŸ’¾ Auto-Save")
        st.markdown("""
        **Automatic Background Saving:**
        - ğŸ“„ Graph State (JSON)
        - ğŸ“Š Graph Visualizations (PNG/TXT)
        - ğŸ“ Output Directory: `./output/`
        """)
        
        if st.button("ğŸ”„ Reset Session"):
            cleanup_temp_files()
            for key in st.session_state.keys():
                del st.session_state[key]
            st.rerun()
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    display_file_upload_section()
    
    st.markdown("---")
    
    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("### ğŸ’¬ Ask a Question")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Enter your semiconductor physics question..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘
        with st.chat_message("assistant"):
            # íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ í™•ì¸ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
            pdf_file = st.session_state.uploaded_files.get("pdf")
            image_files = st.session_state.uploaded_files.get("images", [])
            
            # ì²˜ë¦¬ ì‹¤í–‰
            try:
                # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼
                try:
                    current_loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    
                    def run_async_in_thread():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            if pdf_file or image_files:
                                return new_loop.run_until_complete(
                                    process_query_with_files(prompt, pdf_file, image_files)
                                )
                            else:
                                return new_loop.run_until_complete(
                                    process_query_with_langgraph_streaming(prompt)
                                )
                        except Exception as thread_error:
                            print(f"âŒ Error in thread execution: {thread_error}")
                            print(traceback.format_exc())
                            raise thread_error
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result()
                        
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    try:
                        if pdf_file or image_files:
                            result = asyncio.run(process_query_with_files(prompt, pdf_file, image_files))
                        else:
                            result = asyncio.run(process_query_with_langgraph_streaming(prompt))
                    except Exception as direct_error:
                        print(f"âŒ Error in direct execution: {direct_error}")
                        print(traceback.format_exc())
                        raise direct_error
                
                # ê²°ê³¼ ì²˜ë¦¬ ë° í‘œì‹œ
                if result:
                    answer = result.get("answer", "No answer generated")
                    complexity = "Complex" if result.get("executed_steps") else "Simple"
                    
                    # íŒŒì¼ ì²˜ë¦¬ ì •ë³´ëŠ” ê°„ë‹¨í•˜ê²Œë§Œ í‘œì‹œ (ì„ íƒì )
                    file_info = ""
                    if pdf_file or image_files:
                        file_count = (1 if pdf_file else 0) + len(image_files)
                        file_info = f" (with {file_count} uploaded file{'s' if file_count > 1 else ''})"
                    
                    response_content = f"**Question Complexity:** {complexity}{file_info}\n\n**Answer:** {answer}"
                    
                    # ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ìë™ ì €ì¥ (ì‚¬ìš©ìì—ê²Œ ë³´ì´ì§€ ì•ŠìŒ)
                    try:
                        # Graph State ì €ì¥
                        saved_state_file = save_graph_state_to_output(result, prompt)
                        if saved_state_file:
                            print(f"ğŸ”„ Background: Graph state saved to {Path(saved_state_file).name}")
                        
                        # Graph ì‹œê°í™” ì €ì¥
                        saved_graph_files = save_graph_visualizations(result, prompt)
                        if saved_graph_files:
                            print(f"ğŸ”„ Background: {len(saved_graph_files)} graph visualization(s) saved")
                            
                    except Exception as save_error:
                        print(f"âŒ Background save error: {save_error}")
                    
                    # íŒŒì¼ ì—…ë¡œë“œ ìƒíƒœ ìë™ ì •ë¦¬
                    if pdf_file or image_files:
                        st.session_state.uploaded_files = {"pdf": None, "images": []}
                        
                else:
                    response_content = "**Error:** Failed to process question"
                    
            except Exception as e:
                error_msg = f"Error processing question: {str(e)}"
                st.error(error_msg)
                print(f"âŒ {error_msg}")
                print(f"âŒ Full traceback:")
                print(traceback.format_exc())
                response_content = f"**Error:** {str(e)}"
            
            # ğŸ”¥ ì €ì¥ ê´€ë ¨ UI ì™„ì „ ì œê±° - ëŒ€ì‹  ê°„ë‹¨í•œ ìƒíƒœ í‘œì‹œë§Œ
            st.markdown("---")
            st.markdown("#### â„¹ï¸ Session Info")
            col_info1, col_info2, col_info3 = st.columns(3)
            
            with col_info1:
                if st.button("ğŸ”„ Refresh", help="Refresh the interface"):
                    st.rerun()
            
            with col_info2:
                output_dir = Path(config.OUTPUT_DIR)
                if output_dir.exists():
                    file_count = len(list(output_dir.glob("*")))
                    st.metric("ğŸ“ Output Files", file_count, help=f"Files in {output_dir}")
                else:
                    st.metric("ğŸ“ Output Files", 0)
            
            with col_info3:
                if st.button("ğŸ—‘ï¸ Clear Files", help="Clear uploaded files"):
                    cleanup_temp_files()
                    st.session_state.uploaded_files = {"pdf": None, "images": []}
                    st.rerun()
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response_content})


# ğŸ”¥ ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ ì¶”ê°€ - ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!
if __name__ == "__main__":
    main()
