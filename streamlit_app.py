import streamlit as st
import sys
import os
from pathlib import Path
import time
import json
from typing import Dict, Any, List, Tuple
import asyncio
import traceback
import tempfile
import shutil
import uuid
from datetime import datetime
import concurrent.futures

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from rag_pipeline.graph_builder import build_graph
from rag_pipeline.graph_state import GraphState
from rag_pipeline import config, utils
from rag_pipeline.plan_execute_langgraph import PlanExecuteLangGraph

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Semiconductor Physics RAG System",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .step-container {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
        background-color: #f8f9fa;
    }
    .step-header {
        font-size: 1.3rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .plan-step {
        background-color: #e3f2fd;
        padding: 0.5rem;
        margin: 0.3rem 0;
        border-left: 4px solid #2196f3;
        border-radius: 4px;
    }
    .context-box {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        max-height: 300px;
        overflow-y: auto;
    }
    .result-box {
        background-color: #e8f5e8;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
    .complexity-simple {
        color: #28a745;
        font-weight: bold;
    }
    .complexity-complex {
        color: #dc3545;
        font-weight: bold;
    }
    .loading-spinner {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid #f3f3f3;
        border-top: 3px solid #3498db;
        border-radius: 50%;
        animation: spin 1s linear infinite;
    }
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
</style>
""", unsafe_allow_html=True)


def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_step" not in st.session_state:
        st.session_state.current_step = 0
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "complexity" not in st.session_state:
        st.session_state.complexity = None
    if "plan_results" not in st.session_state:
        st.session_state.plan_results = []
    if "final_answer" not in st.session_state:
        st.session_state.final_answer = None
    if "uploaded_files" not in st.session_state:
        st.session_state.uploaded_files = {"pdf": None, "images": []}
    if "temp_dir" not in st.session_state:
        st.session_state.temp_dir = None


def save_uploaded_files(uploaded_files, file_type="image"):
    """ì—…ë¡œë“œëœ íŒŒì¼ë“¤ì„ ì„ì‹œ ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    if not uploaded_files:
        return None
    
    # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
    if st.session_state.temp_dir is None:
        st.session_state.temp_dir = tempfile.mkdtemp(prefix="streamlit_rag_")
    
    temp_dir = Path(st.session_state.temp_dir)
    
    if file_type == "pdf":
        # PDF íŒŒì¼ í•˜ë‚˜ë§Œ ì²˜ë¦¬
        pdf_file = uploaded_files
        pdf_path = temp_dir / pdf_file.name
        with open(pdf_path, "wb") as f:
            f.write(pdf_file.getbuffer())
        return pdf_path
    
    elif file_type == "image":
        # ì—¬ëŸ¬ ì´ë¯¸ì§€ íŒŒì¼ ì²˜ë¦¬
        if len(uploaded_files) == 1:
            # ë‹¨ì¼ ì´ë¯¸ì§€ íŒŒì¼
            image_file = uploaded_files[0]
            image_path = temp_dir / image_file.name
            with open(image_path, "wb") as f:
                f.write(image_file.getbuffer())
            return image_path
        else:
            # ì—¬ëŸ¬ ì´ë¯¸ì§€ íŒŒì¼ë“¤ì„ í•˜ë‚˜ì˜ ë””ë ‰í† ë¦¬ì— ì €ì¥
            images_dir = temp_dir / "images"
            images_dir.mkdir(exist_ok=True)
            
            for i, image_file in enumerate(uploaded_files):
                image_path = images_dir / f"{i:03d}_{image_file.name}"
                with open(image_path, "wb") as f:
                    f.write(image_file.getbuffer())
            
            return images_dir
    
    return None


def cleanup_temp_files():
    """ì„ì‹œ íŒŒì¼ë“¤ ì •ë¦¬"""
    if st.session_state.temp_dir and Path(st.session_state.temp_dir).exists():
        try:
            shutil.rmtree(st.session_state.temp_dir)
            st.session_state.temp_dir = None
            print("Temporary files cleaned up")
        except Exception as e:
            print(f"Error cleaning up temp files: {e}")


def display_file_upload_section():
    """íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜ í‘œì‹œ"""
    st.markdown("### ğŸ“ File Upload (Optional)")
    
    col1, col2 = st.columns(2)
    
    with col1:
        st.markdown("**ğŸ“„ PDF Document**")
        uploaded_pdf = st.file_uploader(
            "Upload a PDF file for analysis",
            type=['pdf'],
            key="pdf_uploader",
            help="Upload a PDF document to extract text and analyze with your question"
        )
        
        if uploaded_pdf:
            st.success(f"âœ… PDF uploaded: {uploaded_pdf.name}")
            st.session_state.uploaded_files["pdf"] = uploaded_pdf
        else:
            st.session_state.uploaded_files["pdf"] = None
    
    with col2:
        st.markdown("**ğŸ–¼ï¸ Images**")
        uploaded_images = st.file_uploader(
            "Upload image(s) for analysis",
            type=['png', 'jpg', 'jpeg', 'bmp', 'tiff'],
            accept_multiple_files=True,
            key="image_uploader",
            help="Upload one or more images to extract text and analyze with your question"
        )
        
        if uploaded_images:
            st.success(f"âœ… {len(uploaded_images)} image(s) uploaded")
            for img in uploaded_images:
                st.caption(f"ğŸ“¸ {img.name}")
            st.session_state.uploaded_files["images"] = uploaded_images
        else:
            st.session_state.uploaded_files["images"] = []
    
    # íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ í‘œì‹œ
    if st.session_state.uploaded_files["pdf"] or st.session_state.uploaded_files["images"]:
        st.info("ğŸ’¡ Files will be processed together with your question when submitted.")
        
        # íŒŒì¼ ë¯¸ë¦¬ë³´ê¸° ì˜µì…˜
        with st.expander("ğŸ” File Preview", expanded=False):
            if st.session_state.uploaded_files["pdf"]:
                st.markdown("**PDF File:**")
                st.text(f"ğŸ“„ {st.session_state.uploaded_files['pdf'].name}")
                st.text(f"ğŸ“ Size: {st.session_state.uploaded_files['pdf'].size:,} bytes")
            
            if st.session_state.uploaded_files["images"]:
                st.markdown("**Image Files:**")
                for i, img in enumerate(st.session_state.uploaded_files["images"], 1):
                    col_img1, col_img2 = st.columns([3, 1])
                    with col_img1:
                        st.image(img, caption=f"{i}. {img.name}", width=200)
                    with col_img2:
                        st.text(f"ğŸ“ Size: {img.size:,} bytes")


async def process_query_with_files(query: str, pdf_file=None, image_files=None):
    """íŒŒì¼ê³¼ í•¨ê»˜ ì¿¼ë¦¬ ì²˜ë¦¬ - PDF ì—ëŸ¬ ì²˜ë¦¬ ê°•í™”"""
    
    # íŒŒì¼ ì €ì¥ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²˜ë¦¬)
    pdf_path = None
    img_path = None
    
    try:
        if pdf_file:
            print(f"ğŸ“„ Processing PDF: {pdf_file.name}")
            pdf_path = save_uploaded_files(pdf_file, "pdf")
            print(f"ğŸ“ PDF saved to: {pdf_path}")
            
            # PDF ì²˜ë¦¬ ê°€ëŠ¥ì„± ì‚¬ì „ ì²´í¬
            try:
                # Poppler ì„¤ì¹˜ í™•ì¸
                import subprocess
                result = subprocess.run(['pdftoppm', '-h'], 
                                      capture_output=True, text=True, timeout=5)
                if result.returncode != 0:
                    raise FileNotFoundError("Poppler not accessible")
                    
                print("âœ… Poppler is available for PDF processing")
                
            except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as poppler_error:
                error_msg = """
âŒ **PDF Processing Error: Poppler Not Found**

PDF processing requires Poppler to be installed. Please follow these steps:

**Quick Fix:**
1. Download the installer: `install_poppler.py` (in project root)
2. Run: `python install_poppler.py`
3. Restart this application

**Manual Installation:**
1. Visit: https://github.com/oschwartz10612/poppler-windows/releases
2. Download the latest release ZIP
3. Extract to a folder (e.g., `C:\\poppler`)
4. Add `poppler/bin` to your system PATH
5. Restart this application

**Test Installation:**
Open command prompt and run: `pdftoppm -h`

**Alternative (if using conda):**
```bash
conda install -c conda-forge poppler
```
                """
                st.error(error_msg)
                return None
        
        if image_files:
            img_path = save_uploaded_files(image_files, "image")
            print(f"ğŸ–¼ï¸ Images saved to: {img_path}")
        
        # ğŸ”¥ ë‹¨ì¼ LangGraph ì‹¤í–‰ìœ¼ë¡œ í†µí•©
        return await execute_langgraph_once_with_streaming(
            query, pdf_path, img_path
        )
                
    except Exception as e:
        # PDF ê´€ë ¨ ì—ëŸ¬ëŠ” ë” ìì„¸í•œ ì •ë³´ ì œê³µ
        if "poppler" in str(e).lower() or "pdftoppm" in str(e).lower():
            st.error("âŒ **PDF Processing Error**")
            st.error("Poppler is required for PDF processing but not found.")
            
            with st.expander("ğŸ”§ **How to Fix This Error**", expanded=True):
                st.markdown("""
                **Method 1: Quick Install**
                1. Download `install_poppler.py` from the project root
                2. Run in terminal: `python install_poppler.py`
                3. Restart this application
                
                **Method 2: Manual Install**
                1. Go to: https://github.com/oschwartz10612/poppler-windows/releases
                2. Download the latest ZIP file
                3. Extract to `C:\\poppler`
                4. Add `C:\\poppler\\bin` to system PATH
                5. Restart this application
                
                **Method 3: Using Conda**
                ```bash
                conda install -c conda-forge poppler
                ```
                
                **Test Installation:**
                Open command prompt and run: `pdftoppm -h`
                """)
        else:
            st.error(f"âŒ Error processing query with files: {str(e)}")
        
        print(f"âŒ Full error traceback:")
        print(traceback.format_exc())
        return None


def debug_graph_execution(graph, init_state: GraphState):
    """ê·¸ë˜í”„ ì‹¤í–‰ ë””ë²„ê¹…ì„ ìœ„í•œ í•¨ìˆ˜"""
    print("ğŸ” Debug: Testing graph structure...")
    
    try:
        # ê·¸ë˜í”„ êµ¬ì¡° í™•ì¸
        graph_dict = graph.get_graph()
        print(f"   Graph nodes: {list(graph_dict.nodes.keys())}")
        print(f"   Graph edges: {[(e.source, e.target) for e in graph_dict.edges]}")
        
        # ë‹¨ìˆœ invoke í…ŒìŠ¤íŠ¸
        print("ğŸ” Debug: Testing simple invoke...")
        test_result = graph.invoke(init_state)
        print(f"   Invoke result keys: {list(test_result.keys()) if isinstance(test_result, dict) else 'non-dict'}")
        print(f"   Invoke answer: {test_result.get('answer', 'NO ANSWER')[:100]}...")
        
        return test_result
        
    except Exception as e:
        print(f"âŒ Debug error: {e}")
        return None


async def execute_langgraph_once_with_streaming(
    query: str, 
    pdf_path=None, 
    img_path=None
) -> Dict[str, Any]:
    """
    ğŸ”¥ í•µì‹¬ ìˆ˜ì •: LangGraphë¥¼ í•œ ë²ˆë§Œ ì‹¤í–‰í•˜ì—¬ ì¤‘ê°„ ê²°ê³¼ì™€ ìµœì¢… ê²°ê³¼ë¥¼ ëª¨ë‘ ìˆ˜ì§‘
    """
    print("ğŸš€ Starting unified LangGraph execution...")
    
    try:
        # ê·¸ë˜í”„ ë¹Œë“œ
        graph = build_graph(
            pdf_path=pdf_path,
            img_path=img_path,
            retrieval_type=config.RETRIEVAL_TYPE,
            hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
        )
        
        # ì´ˆê¸° ìƒíƒœ ì„¤ì •
        init_state: GraphState = {"question": [query], "messages": [("user", query)]}
        
        # ğŸ”¥ ë””ë²„ê¹…ìš© ê·¸ë˜í”„ í…ŒìŠ¤íŠ¸ (ê°œë°œ ì¤‘ì—ë§Œ ì‚¬ìš©)
        if st.session_state.get("debug_mode", False):
            debug_result = debug_graph_execution(graph, init_state)
        
        # ğŸ”¥ ì¤‘ê°„ ê²°ê³¼ ìˆ˜ì§‘ì„ ìœ„í•œ ë³€ìˆ˜ë“¤
        intermediate_results = []
        final_state = None
        accumulated_state = {}  # ğŸ”¥ ìƒíƒœ ëˆ„ì ìš©
        
        # ğŸ”¥ ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰í•˜ë©´ì„œ ì¤‘ê°„ ê²°ê³¼ ìˆ˜ì§‘
        print("ğŸ“¡ Starting streaming execution...")
        node_count = 0
        
        async for event in graph.astream(init_state):
            print(f"ğŸ” Event {node_count + 1}: {list(event.keys())}")
            
            for node_name, node_output in event.items():
                if node_name == "__end__":
                    # ìµœì¢… ìƒíƒœëŠ” ëˆ„ì ëœ ìƒíƒœë¥¼ ì‚¬ìš©
                    final_state = accumulated_state.copy()
                    print(f"âœ… Final state captured with keys: {list(final_state.keys())}")
                    continue
                
                # ğŸ”¥ ìƒíƒœ ëˆ„ì  (ê° ë…¸ë“œì˜ ì¶œë ¥ì„ ëˆ„ì )
                if isinstance(node_output, dict):
                    accumulated_state.update(node_output)
                    print(f"   Node '{node_name}' added keys: {list(node_output.keys())}")
                    print(f"   Accumulated keys: {list(accumulated_state.keys())}")
                
                # ì¤‘ê°„ ê²°ê³¼ ì €ì¥
                intermediate_results.append({
                    "node_name": node_name,
                    "node_output": node_output,
                    "timestamp": asyncio.get_event_loop().time()
                })
                
                # ì‹¤ì‹œê°„ UI ì—…ë°ì´íŠ¸
                await display_node_execution_unified(node_name, node_output, query)
                
                # ì‚¬ìš©ìê°€ ì½ì„ ìˆ˜ ìˆë„ë¡ ì ì‹œ ëŒ€ê¸°
                await asyncio.sleep(1)
                node_count += 1
        
        # ğŸ”¥ ìµœì¢… ìƒíƒœ í™•ì¸ ë° ì²˜ë¦¬
        if final_state is None or not final_state:
            print("âš ï¸ No proper final state, using accumulated state...")
            final_state = accumulated_state
        
        # ğŸ”¥ í•„ìˆ˜ í‚¤ê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ì„¤ì •
        essential_keys = ["answer", "context", "executed_steps", "plan", "combined_context", "explanation"]
        for key in essential_keys:
            if key not in final_state:
                if key == "answer":
                    final_state[key] = "Processing completed but no final answer generated"
                elif key in ["context", "executed_steps", "plan"]:
                    final_state[key] = []
                else:
                    final_state[key] = ""
        
        print(f"ğŸ” Final state summary:")
        print(f"   Answer: {final_state.get('answer', 'None')[:100]}...")
        print(f"   Context docs: {len(final_state.get('context', []))}")
        print(f"   Executed steps: {len(final_state.get('executed_steps', []))}")
        print(f"   Plan: {len(final_state.get('plan', []))}")
        
        # ğŸ”¥ ìˆ˜ì§‘ëœ ê²°ê³¼ë¥¼ ì •ë¦¬í•˜ì—¬ ë°˜í™˜
        processed_result = {
            "answer": final_state.get("answer", "No answer generated"),
            "context": final_state.get("context", []),
            "executed_steps": final_state.get("executed_steps", []),
            "plan": final_state.get("plan", []),
            "combined_context": final_state.get("combined_context", ""),
            "explanation": final_state.get("explanation", ""),
            "intermediate_results": intermediate_results,
            "node_count": len(intermediate_results),
            "complexity": "Complex" if final_state.get("executed_steps") else "Simple"
        }
        
        print(f"âœ… Unified execution completed: {len(intermediate_results)} intermediate steps")
        return processed_result
        
    except Exception as e:
        print(f"âŒ Error in unified LangGraph execution: {e}")
        print(traceback.format_exc())
        return {
            "answer": f"Error in processing: {str(e)}",
            "context": [],
            "executed_steps": [],
            "plan": [],
            "combined_context": "",
            "explanation": f"Error: {str(e)}",
            "intermediate_results": [],
            "node_count": 0,
            "complexity": "Error"
        }


async def process_query_with_langgraph_streaming(query: str):
    """ğŸ”¥ í†µí•©ëœ LangGraph ì‹¤í–‰ - íŒŒì¼ ì—†ëŠ” ê²½ìš°"""
    return await execute_langgraph_once_with_streaming(query, None, None)


async def display_node_execution_unified(node_name: str, node_output: Dict[str, Any], query: str):
    """ğŸ”¥ ìˆ˜ì •ëœ ë…¸ë“œ ì‹¤í–‰ í‘œì‹œ - ìµœì¢… ê²°ê³¼ í‘œì‹œ ì—†ì´ ì¤‘ê°„ ë‹¨ê³„ë§Œ"""
    
    if node_name == "complexity_check":
        # ë³µì¡ë„ ì²´í¬ ê²°ê³¼ í‘œì‹œ
        complexity = node_output.get("next", "unknown")
        st.markdown("### ğŸ” Step 1: Analyzing Question Complexity")
        display_complexity_result(complexity)
        
        if complexity == "simple":
            st.info("ğŸ”„ Proceeding with simple retrieval pipeline...")
        else:
            st.info("ğŸ§  Proceeding with complex Plan-and-Execute pipeline...")
            
    elif node_name in ["retrieve_simple", "extract_file_content"]:
        # Simple ê²€ìƒ‰ ë‹¨ê³„
        st.markdown("### ğŸ“š Step 2: Information Retrieval")
        with st.spinner("Searching relevant documents..."):
            await asyncio.sleep(0.5)  # ì‹œê°ì  íš¨ê³¼
        
        context = node_output.get("context", [])
        st.success(f"âœ… Retrieved {len(context)} relevant documents")
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
        if context:
            with st.expander("ğŸ“– Preview Retrieved Documents", expanded=False):
                for i, doc in enumerate(context[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ
                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                    st.markdown(f"**Document {i}:**")
                    st.markdown(f'<div class="context-box">{content[:300]}...</div>', 
                              unsafe_allow_html=True)
    
    elif node_name == "relevance_check":
        # ê´€ë ¨ì„± ì²´í¬ ë‹¨ê³„
        st.markdown("### ğŸ¯ Step 3: Relevance Filtering")
        with st.spinner("Filtering relevant content..."):
            await asyncio.sleep(0.5)
        
        filtered_context = node_output.get("filtered_context", [])
        scores = node_output.get("scores", [])
        
        if scores:
            avg_score = sum(scores) / len(scores)
            st.success(f"âœ… Filtered content (avg similarity: {avg_score:.3f})")
        else:
            st.success("âœ… Content filtering completed")
    
    elif node_name == "llm_answer_simple":
        # Simple ë‹µë³€ ìƒì„±
        st.markdown("### ğŸ’¡ Step 4: Generating Answer")
        with st.spinner("Generating final answer..."):
            await asyncio.sleep(1)
        
        st.success("âœ… Answer generated successfully")
        
    elif node_name == "plan_and_execute":
        # Complex ì²˜ë¦¬ ê²°ê³¼
        st.markdown("### ğŸ§  Complex Query Processing Results")
        
        # Plan í‘œì‹œ
        plan = node_output.get("plan", [])
        if plan:
            st.markdown("**ğŸ“‹ Execution Plan:**")
            for i, step in enumerate(plan, 1):
                st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                          unsafe_allow_html=True)
        
        # ì‹¤í–‰ëœ ë‹¨ê³„ë“¤ í‘œì‹œ
        executed_steps = node_output.get("executed_steps", [])
        if executed_steps:
            with st.expander(f"ğŸ“Š Executed Steps ({len(executed_steps)} steps)", expanded=False):
                for i, (step, result) in enumerate(executed_steps, 1):
                    st.markdown(f"**Step {i}:** {step}")
                    st.markdown(f"*Result:* {result[:200]}...")
                    st.markdown("---")
        
        # ì»¨í…ìŠ¤íŠ¸ëŠ” ìµœì¢… ê²°ê³¼ì—ì„œë§Œ í‘œì‹œí•˜ë„ë¡ ìƒëµ
        st.success("âœ… Complex processing completed")


def display_final_results_unified(processed_result: Dict[str, Any], query: str):
    """ğŸ”¥ í†µí•©ëœ ìµœì¢… ê²°ê³¼ í‘œì‹œ - ì´ë¯¸ ìˆ˜ì§‘ëœ ê²°ê³¼ ì‚¬ìš©"""
    st.markdown("---")
    st.markdown("### ğŸ‰ Final Results")
    
    # ìµœì¢… ë‹µë³€
    answer = processed_result.get("answer", "No answer generated")
    st.markdown("#### ğŸ’¡ Answer:")
    st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
    
    # ì²˜ë¦¬ ê²½ë¡œ ìš”ì•½
    explanation = processed_result.get("explanation", "")
    if explanation:
        st.markdown("#### ğŸ“ Processing Summary:")
        st.info(explanation)
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        context_count = len(processed_result.get("context", []))
        st.metric("ğŸ“š Documents Retrieved", context_count)
    
    with col2:
        # Plan-Execute ì •ë³´
        complexity = processed_result.get("complexity", "Unknown")
        if complexity == "Complex":
            executed_steps = processed_result.get("executed_steps", [])
            st.metric("ğŸ”„ Steps Executed", len(executed_steps))
        else:
            st.metric("ğŸ” Processing Type", "Simple Retrieval")
    
    with col3:
        node_count = processed_result.get("node_count", 0)
        st.metric("âš™ï¸ Graph Nodes", node_count)
    
    # ğŸ”¥ ë°±ê·¸ë¼ìš´ë“œ ìë™ ì €ì¥ (ê¸°ì¡´ í•¨ìˆ˜ ì¬ì‚¬ìš©)
    try:
        # processed_resultë¥¼ final_state í˜•ì‹ìœ¼ë¡œ ë³€í™˜
        final_state_for_save = {
            "answer": processed_result.get("answer", ""),
            "context": processed_result.get("context", []),
            "executed_steps": processed_result.get("executed_steps", []),
            "plan": processed_result.get("plan", []),
            "combined_context": processed_result.get("combined_context", ""),
            "explanation": processed_result.get("explanation", ""),
        }
        
        # Graph State ì €ì¥
        saved_state_file = save_graph_state_to_output(final_state_for_save, query)
        if saved_state_file:
            print(f"âœ… Graph state auto-saved to: {saved_state_file}")
        
        # Graph ì‹œê°í™” ì €ì¥
        saved_graph_files = save_graph_visualizations(final_state_for_save, query)
        if saved_graph_files:
            print(f"âœ… Graph visualizations auto-saved:")
            for graph_file in saved_graph_files:
                print(f"   ğŸ“ˆ {Path(graph_file).name}")
        
    except Exception as e:
        print(f"âŒ Error in background auto-save: {e}")


def display_complexity_result(complexity: str):
    """ë³µì¡ë„ íŒì • ê²°ê³¼ í‘œì‹œ"""
    if complexity == "simple":
        st.markdown(
            f'<div class="complexity-simple">ğŸ“ Question Complexity: SIMPLE</div>',
            unsafe_allow_html=True
        )
        st.info("This question will be processed using direct retrieval.")
    else:
        st.markdown(
            f'<div class="complexity-complex">ğŸ§  Question Complexity: COMPLEX</div>',
            unsafe_allow_html=True
        )
        st.info("This question requires multi-step reasoning and will be processed using Plan-and-Execute.")


def save_graph_state_to_output(final_state: Dict[str, Any], query: str) -> str:
    """ìµœì¢… GraphStateë¥¼ output ë””ë ‰í† ë¦¬ì— ì €ì¥"""
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        # GraphState í˜•ì‹ì— ë§ì¶° ë°ì´í„° êµ¬ì„±
        graph_state_dict = {
            "question": [query],
            "explanation": final_state.get("explanation", ""),
            "context": final_state.get("context", []),
            "filtered_context": final_state.get("filtered_context", []),
            "examples": final_state.get("examples", ""),
            "answer": final_state.get("answer", ""),
            "messages": final_state.get("messages", []),
            "scores": final_state.get("scores", []),
            "filtered_scores": final_state.get("filtered_scores", []),
            "subquestions": final_state.get("subquestions", []),
            "subquestion_results": final_state.get("subquestion_results", []),
            "combined_context": final_state.get("combined_context", ""),
            "plan": final_state.get("plan", []),
            "executed_steps": final_state.get("executed_steps", []),
            "retrieval_type": config.RETRIEVAL_TYPE,
            "hybrid_weights": [config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT],
            "next": final_state.get("next", "")
        }
        
        # íŒŒì¼ëª… ìƒì„± (íƒ€ì„ìŠ¤íƒ¬í”„ + UUID)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        unique_id = str(uuid.uuid4())[:8]
        filename = f"graph_state_{timestamp}_{unique_id}.json"
        file_path = output_dir / filename
        
        # JSONìœ¼ë¡œ ì €ì¥ (ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜)
        serializable_state = convert_to_serializable(graph_state_dict)
        
        with open(file_path, "w", encoding="utf-8") as f:
            json.dump(serializable_state, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… Graph state saved to: {file_path}")
        return str(file_path)
        
    except Exception as e:
        print(f"âŒ Error saving graph state: {e}")
        return ""


def convert_to_serializable(obj):
    """ê°ì²´ë¥¼ JSON ì§ë ¬í™” ê°€ëŠ¥í•œ í˜•íƒœë¡œ ë³€í™˜"""
    if hasattr(obj, 'page_content'):  # Document ê°ì²´
        return {
            "page_content": obj.page_content,
            "metadata": getattr(obj, 'metadata', {})
        }
    elif hasattr(obj, 'content'):  # Message ê°ì²´
        return obj.content
    elif isinstance(obj, list):
        return [convert_to_serializable(item) for item in obj]
    elif isinstance(obj, dict):
        return {key: convert_to_serializable(value) for key, value in obj.items()}
    elif isinstance(obj, tuple):
        return [convert_to_serializable(item) for item in obj]
    else:
        return obj


def save_graph_visualizations(final_state: Dict[str, Any], query: str) -> List[str]:
    """ë‘ ê°€ì§€ ê·¸ë˜í”„ë¥¼ ì‹œê°í™”í•˜ì—¬ ì €ì¥"""
    saved_files = []
    
    try:
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ í™•ì¸
        output_dir = Path(config.OUTPUT_DIR)
        output_dir.mkdir(parents=True, exist_ok=True)
        
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        
        # 1. Main Graph (graph_builder.pyì—ì„œ ìƒì„±ëœ ê·¸ë˜í”„) ì‹œê°í™”
        try:
            print("ğŸ“Š Generating main graph visualization...")
            
            # ë™ì¼í•œ ì„¤ì •ìœ¼ë¡œ ê·¸ë˜í”„ ì¬ìƒì„±
            pdf_file = st.session_state.uploaded_files.get("pdf")
            image_files = st.session_state.uploaded_files.get("images", [])
            
            pdf_path = None
            img_path = None
            if pdf_file:
                pdf_path = save_uploaded_files(pdf_file, "pdf")
            if image_files:
                img_path = save_uploaded_files(image_files, "image")
            
            main_graph = build_graph(
                pdf_path=pdf_path,
                img_path=img_path,
                retrieval_type=config.RETRIEVAL_TYPE,
                hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
            )
            
            main_graph_path = output_dir / f"main_graph_{timestamp}.png"
            main_graph.get_graph().draw_mermaid_png(output_file_path=str(main_graph_path))
            
            saved_files.append(str(main_graph_path))
            print(f"   âœ… Main graph saved to: {main_graph_path}")
            
        except Exception as e:
            print(f"   âŒ Error saving main graph: {e}")
        
        # 2. Plan-Execute Graph ì‹œê°í™” (ë³µì¡í•œ ì§ˆë¬¸ì¸ ê²½ìš°ë§Œ)
        if final_state.get("executed_steps") or final_state.get("plan"):
            try:
                print("ğŸ“Š Generating plan-execute graph visualization...")
                
                from rag_pipeline.plan_execute_langgraph import PlanExecuteLangGraph
                plan_execute_agent = PlanExecuteLangGraph()
                
                plan_execute_graph_path = output_dir / f"plan_execute_graph_{timestamp}.png"
                plan_execute_agent.visualize_graph(output_path=plan_execute_graph_path)
                
                saved_files.append(str(plan_execute_graph_path))
                print(f"   âœ… Plan-execute graph saved to: {plan_execute_graph_path}")
                
            except Exception as e:
                print(f"   âŒ Error saving plan-execute graph: {e}")
        
        return saved_files
        
    except Exception as e:
        print(f"âŒ Error in graph visualization: {e}")
        return []


def display_final_results(final_state: Dict[str, Any], query: str):
    """ê¸°ì¡´ì˜ ìµœì¢… ê²°ê³¼ í‘œì‹œ í•¨ìˆ˜ - ë°±ì—…ìš©"""
    st.markdown("---")
    st.markdown("### ğŸ‰ Final Results")
    
    # ìµœì¢… ë‹µë³€
    answer = final_state.get("answer", "No answer generated")
    st.markdown("#### ğŸ’¡ Answer:")
    st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
    
    # ì²˜ë¦¬ ê²½ë¡œ ìš”ì•½
    explanation = final_state.get("explanation", "")
    if explanation:
        st.markdown("#### ğŸ“ Processing Summary:")
        st.info(explanation)
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        context_count = len(final_state.get("context", []))
        st.metric("ğŸ“š Documents Retrieved", context_count)
    
    with col2:
        # Plan-Execute ì •ë³´
        executed_steps = final_state.get("executed_steps", [])
        if executed_steps:
            st.metric("ğŸ”„ Steps Executed", len(executed_steps))
        else:
            st.metric("ğŸ” Processing Type", "Simple Retrieval")
    
    with col3:
        scores = final_state.get("scores", [])
        if scores and len(scores) > 0:
            avg_score = sum(scores) / len(scores)
            st.metric("ğŸ¯ Avg Similarity", f"{avg_score:.3f}")


# ğŸ”¥ main í•¨ìˆ˜ì—ì„œ response_content ì •ì˜ ë¬¸ì œ ìˆ˜ì •
def main():
    """ë©”ì¸ Streamlit ì•±"""
    initialize_session_state()
    
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ”¬ Semiconductor Physics RAG System</div>', 
              unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("### âš™ï¸ Configuration")
        st.markdown(f"**Model:** {config.OPENAI_MODEL}")
        st.markdown(f"**Retrieval Type:** {config.RETRIEVAL_TYPE}")
        st.markdown(f"**TOP_K:** {config.TOP_K}")
        st.markdown(f"**Max Plan Steps:** {config.MAX_PLAN_STEPS}")
        
        # ğŸ”¥ ë””ë²„ê·¸ ëª¨ë“œ í† ê¸€ ì¶”ê°€
        debug_mode = st.checkbox("ğŸ› Debug Mode", value=False, help="Enable detailed debugging logs")
        st.session_state["debug_mode"] = debug_mode
        
        st.markdown("### ğŸ“Š Graph Flow")
        st.markdown("""
        **LangGraph Execution:**
        1. ğŸ” Complexity Check
        2a. ğŸ“š Simple: Retrieve â†’ Filter â†’ Answer
        2b. ğŸ§  Complex: Plan-and-Execute
        3. ğŸ’¡ Final Answer
        """)
        
        st.markdown("### ğŸ“ File Processing")
        st.markdown("""
        **Supported Files:**
        - ğŸ“„ PDF documents
        - ğŸ–¼ï¸ Images (PNG, JPG, etc.)
        
        **Processing Flow:**
        1. File Upload
        2. Text Extraction (OCR/PDF parsing)
        3. Content Integration
        4. Query Processing
        """)
        
        # ğŸ”¥ ìë™ ì €ì¥ ì •ë³´ ì¶”ê°€
        st.markdown("### ğŸ’¾ Auto-Save")
        st.markdown("""
        **Automatic Background Saving:**
        - ğŸ“„ Graph State (JSON)
        - ğŸ“Š Graph Visualizations (PNG/TXT)
        - ğŸ“ Output Directory: `./output/`
        """)
        
        if st.button("ğŸ”„ Reset Session"):
            cleanup_temp_files()
            for key in st.session_state.keys():
                del st.session_state[key]
            st.rerun()
    
    # íŒŒì¼ ì—…ë¡œë“œ ì„¹ì…˜
    display_file_upload_section()
    
    st.markdown("---")
    
    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("### ğŸ’¬ Ask a Question")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Enter your semiconductor physics question..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘
        with st.chat_message("assistant"):
            # íŒŒì¼ ì²˜ë¦¬ ìƒíƒœ í™•ì¸ (ë°±ê·¸ë¼ìš´ë“œì—ì„œ)
            pdf_file = st.session_state.uploaded_files.get("pdf")
            image_files = st.session_state.uploaded_files.get("images", [])
            
            # ğŸ”¥ response_content ì´ˆê¸°í™”
            response_content = "**Error:** Initialization failed"
            
            # ğŸ”¥ í†µí•©ëœ ì²˜ë¦¬ ì‹¤í–‰
            try:
                # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼
                try:
                    current_loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    
                    def run_async_in_thread():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            if pdf_file or image_files:
                                return new_loop.run_until_complete(
                                    process_query_with_files(prompt, pdf_file, image_files)
                                )
                            else:
                                return new_loop.run_until_complete(
                                    process_query_with_langgraph_streaming(prompt)
                                )
                        except Exception as thread_error:
                            print(f"âŒ Error in thread execution: {thread_error}")
                            print(traceback.format_exc())
                            raise thread_error
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result()
                        
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    try:
                        if pdf_file or image_files:
                            result = asyncio.run(process_query_with_files(prompt, pdf_file, image_files))
                        else:
                            result = asyncio.run(process_query_with_langgraph_streaming(prompt))
                    except Exception as direct_error:
                        print(f"âŒ Error in direct execution: {direct_error}")
                        print(traceback.format_exc())
                        raise direct_error
                
                # ğŸ”¥ ê²°ê³¼ ì²˜ë¦¬ ë° í‘œì‹œ - í†µí•©ëœ ë°©ì‹ ì‚¬ìš©
                if result:
                    answer = result.get("answer", "No answer generated")
                    complexity = result.get("complexity", "Unknown")
                    
                    # íŒŒì¼ ì²˜ë¦¬ ì •ë³´ëŠ” ê°„ë‹¨í•˜ê²Œë§Œ í‘œì‹œ (ì„ íƒì )
                    file_info = ""
                    if pdf_file or image_files:
                        file_count = (1 if pdf_file else 0) + len(image_files)
                        file_info = f" (with {file_count} uploaded file{'s' if file_count > 1 else ''})"
                    
                    response_content = f"**Question Complexity:** {complexity}{file_info}\n\n**Answer:** {answer}"
                    
                    # ğŸ”¥ í†µí•©ëœ ìµœì¢… ê²°ê³¼ í‘œì‹œ
                    display_final_results_unified(result, prompt)
                    
                    # íŒŒì¼ ì—…ë¡œë“œ ìƒíƒœ ìë™ ì •ë¦¬
                    if pdf_file or image_files:
                        st.session_state.uploaded_files = {"pdf": None, "images": []}
                        
                else:
                    response_content = "**Error:** Failed to process question"
                    
            except Exception as e:
                error_msg = f"Error processing question: {str(e)}"
                st.error(error_msg)
                print(f"âŒ {error_msg}")
                print(f"âŒ Full traceback:")
                print(traceback.format_exc())
                response_content = f"**Error:** {str(e)}"
            
            # ğŸ”¥ ì €ì¥ ê´€ë ¨ UI ì™„ì „ ì œê±° - ëŒ€ì‹  ê°„ë‹¨í•œ ìƒíƒœ í‘œì‹œë§Œ
            st.markdown("---")
            st.markdown("#### â„¹ï¸ Session Info")
            col_info1, col_info2, col_info3 = st.columns(3)
            
            with col_info1:
                if st.button("ğŸ”„ Refresh", help="Refresh the interface"):
                    st.rerun()
            
            with col_info2:
                output_dir = Path(config.OUTPUT_DIR)
                if output_dir.exists():
                    file_count = len(list(output_dir.glob("*")))
                    st.metric("ğŸ“ Output Files", file_count, help=f"Files in {output_dir}")
                else:
                    st.metric("ğŸ“ Output Files", 0)
            
            with col_info3:
                if st.button("ğŸ—‘ï¸ Clear Files", help="Clear uploaded files"):
                    cleanup_temp_files()
                    st.session_state.uploaded_files = {"pdf": None, "images": []}
                    st.rerun()
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì €ì¥
        st.session_state.messages.append({"role": "assistant", "content": response_content})


# ğŸ”¥ ë©”ì¸ í•¨ìˆ˜ í˜¸ì¶œ ì¶”ê°€ - ì´ ë¶€ë¶„ì´ ëˆ„ë½ë˜ì–´ ìˆì—ˆìŒ!
if __name__ == "__main__":
    main()
