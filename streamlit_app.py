import streamlit as st
import sys
import os
from pathlib import Path
import time
import json
from typing import Dict, Any, List, Tuple
import asyncio
import traceback

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ Python ê²½ë¡œì— ì¶”ê°€
sys.path.append(str(Path(__file__).parent))

from rag_pipeline.graph_builder import build_graph
from rag_pipeline.graph_state import GraphState
from rag_pipeline import config, utils
from rag_pipeline.plan_execute_langgraph import PlanExecuteLangGraph

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(
    page_title="Semiconductor Physics RAG System",
    page_icon="ğŸ”¬",
    layout="wide",
    initial_sidebar_state="expanded"
)

# ìŠ¤íƒ€ì¼ë§
st.markdown("""
<style>
    .main-header {
        font-size: 2.5rem;
        font-weight: bold;
        text-align: center;
        color: #1f77b4;
        margin-bottom: 2rem;
    }
    .step-container {
        border: 2px solid #e0e0e0;
        border-radius: 10px;
        padding: 1rem;
        margin: 1rem 0;
        background-color: #f8f9fa;
    }
    .step-header {
        font-size: 1.3rem;
        font-weight: bold;
        color: #2c3e50;
        margin-bottom: 0.5rem;
    }
    .plan-step {
        background-color: #e3f2fd;
        padding: 0.5rem;
        margin: 0.3rem 0;
        border-left: 4px solid #2196f3;
        border-radius: 4px;
    }
    .context-box {
        background-color: #f5f5f5;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
        max-height: 300px;
        overflow-y: auto;
    }
    .result-box {
        background-color: #e8f5e8;
        padding: 1rem;
        border-radius: 8px;
        margin: 0.5rem 0;
    }
    .complexity-simple {
        color: #28a745;
        font-weight: bold;
    }
    .complexity-complex {
        color: #dc3545;
        font-weight: bold;
    }
    .loading-spinner {
        display: inline-block;
        width: 20px;
        height: 20px;
        border: 3px solid #f3f3f3;
        border-top: 3px solid #3498db;
        border-radius: 50%;
        animation: spin 1s linear infinite;
    }
    @keyframes spin {
        0% { transform: rotate(0deg); }
        100% { transform: rotate(360deg); }
    }
</style>
""", unsafe_allow_html=True)


def initialize_session_state():
    """ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”"""
    if "messages" not in st.session_state:
        st.session_state.messages = []
    if "current_step" not in st.session_state:
        st.session_state.current_step = 0
    if "processing" not in st.session_state:
        st.session_state.processing = False
    if "complexity" not in st.session_state:
        st.session_state.complexity = None
    if "plan_results" not in st.session_state:
        st.session_state.plan_results = []
    if "final_answer" not in st.session_state:
        st.session_state.final_answer = None


def display_complexity_result(complexity: str):
    """ë³µì¡ë„ íŒì • ê²°ê³¼ í‘œì‹œ"""
    if complexity == "simple":
        st.markdown(
            f'<div class="complexity-simple">ğŸ“ Question Complexity: SIMPLE</div>',
            unsafe_allow_html=True
        )
        st.info("This question will be processed using direct retrieval.")
    else:
        st.markdown(
            f'<div class="complexity-complex">ğŸ§  Question Complexity: COMPLEX</div>',
            unsafe_allow_html=True
        )
        st.info("This question requires multi-step reasoning and will be processed using Plan-and-Execute.")


def display_simple_processing(query: str):
    """Simple ì§ˆë¬¸ ì²˜ë¦¬ ê³¼ì • í‘œì‹œ"""
    st.markdown("### ğŸ” Processing Simple Query")
    
    progress_bar = st.progress(0)
    status_text = st.empty()
    
    try:
        # ê·¸ë˜í”„ êµ¬ì„±
        status_text.text("Building retrieval pipeline...")
        progress_bar.progress(20)
        
        graph = build_graph(
            pdf_path=None,
            img_path=None,
            retrieval_type=config.RETRIEVAL_TYPE,
            hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
        )
        
        # ì§ˆë¬¸ ì²˜ë¦¬
        status_text.text("Processing query...")
        progress_bar.progress(50)
        
        init_state: GraphState = {"question": [query], "messages": [("user", query)]}
        final_state = graph.invoke(init_state)
        
        progress_bar.progress(80)
        status_text.text("Generating final answer...")
        
        # ê²°ê³¼ í‘œì‹œ
        progress_bar.progress(100)
        status_text.text("Complete!")
        time.sleep(0.5)
        
        # í”„ë¡œê·¸ë ˆìŠ¤ ë°” ì •ë¦¬
        progress_bar.empty()
        status_text.empty()
        
        # ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
        if "context" in final_state and final_state["context"]:
            with st.expander("ğŸ“š Retrieved Context", expanded=False):
                contexts = final_state["context"]
                for i, doc in enumerate(contexts[:3], 1):  # ì²˜ìŒ 3ê°œë§Œ í‘œì‹œ
                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                    st.markdown(f"**Context {i}:**")
                    st.markdown(f'<div class="context-box">{content[:500]}...</div>', 
                              unsafe_allow_html=True)
        
        # ìµœì¢… ë‹µë³€ í‘œì‹œ
        st.markdown("### ğŸ’¡ Final Answer")
        answer = final_state.get("answer", "No answer generated")
        st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
        
        return final_state
        
    except Exception as e:
        st.error(f"Error processing simple query: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def process_complex_query_step_by_step(query: str):
    """Complex ì§ˆë¬¸ì„ ë‹¨ê³„ë³„ë¡œ ì²˜ë¦¬í•˜ê³  ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸"""
    
    # Plan-and-Execute ì—ì´ì „íŠ¸ ìƒì„±
    agent = PlanExecuteLangGraph()
    
    # ì´ˆê¸° ìƒíƒœ ì„¤ì •
    initial_state = {
        "input": query,
        "plan": [],
        "past_steps": [],
        "response": "",
        "current_step_index": 0,
        "retrieval_type": config.RETRIEVAL_TYPE,
        "hybrid_weights": [config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT],
        "all_context_docs": [],
        "combined_context": ""
    }
    
    # í”„ë¡œê·¸ë ˆìŠ¤ ì¶”ì ì„ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
    step_container = st.empty()
    
    try:
        # Step 1: Planning
        with step_container.container():
            st.markdown("### ğŸ“‹ Step 1: Creating Execution Plan")
            with st.spinner("Planning execution steps..."):
                plan_result = await agent.plan_step(initial_state)
                initial_state.update(plan_result)
            
            # ê³„íš í‘œì‹œ
            if initial_state["plan"]:
                st.markdown("**Generated Plan:**")
                for i, step in enumerate(initial_state["plan"], 1):
                    st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                              unsafe_allow_html=True)
            
            time.sleep(2)  # ì‚¬ìš©ìê°€ ê³„íšì„ ì½ì„ ì‹œê°„
        
        # Step 2+: Execution Loop
        step_num = 2
        max_iterations = config.MAX_PLAN_STEPS + 2  # ì•ˆì „ ì¥ì¹˜
        
        while step_num <= max_iterations:
            current_step_index = initial_state.get("current_step_index", 0)
            plan = initial_state.get("plan", [])
            
            # ì‹¤í–‰í•  ë‹¨ê³„ê°€ ìˆëŠ”ì§€ í™•ì¸
            if current_step_index >= len(plan):
                break
                
            # í˜„ì¬ ë‹¨ê³„ í‘œì‹œ
            with step_container.container():
                current_step = plan[current_step_index]
                st.markdown(f"### ğŸ”„ Step {step_num}: Executing Plan")
                st.markdown(f"**Current Step:** {current_step}")
                
                with st.spinner(f"Executing: {current_step[:50]}..."):
                    # ë‹¨ê³„ ì‹¤í–‰
                    execute_result = await agent.execute_step(initial_state)
                    initial_state.update(execute_result)
                
                # ì‹¤í–‰ ê²°ê³¼ í‘œì‹œ
                past_steps = initial_state.get("past_steps", [])
                if past_steps:
                    latest_step, latest_result = past_steps[-1]
                    
                    col1, col2 = st.columns([1, 1])
                    
                    with col1:
                        st.markdown("**Retrieved Context:**")
                        combined_context = initial_state.get("combined_context", "")
                        if combined_context:
                            # ê°€ì¥ ìµœê·¼ ë‹¨ê³„ì˜ ì»¨í…ìŠ¤íŠ¸ë§Œ í‘œì‹œ
                            recent_context = combined_context.split("=== Step")[-1]
                            st.markdown(f'<div class="context-box">{recent_context[:400]}...</div>', 
                                      unsafe_allow_html=True)
                    
                    with col2:
                        st.markdown("**Step Result:**")
                        st.markdown(f'<div class="result-box">{latest_result[:300]}...</div>', 
                                  unsafe_allow_html=True)
                
                time.sleep(1.5)  # ê²°ê³¼ë¥¼ ì½ì„ ì‹œê°„
            
            # Replan ë‹¨ê³„
            step_num += 1
            with step_container.container():
                st.markdown(f"### ğŸ¤” Step {step_num}: Evaluating Progress")
                
                with st.spinner("Evaluating progress and replanning..."):
                    replan_result = await agent.replan_step(initial_state)
                    initial_state.update(replan_result)
                
                # ì¢…ë£Œ ì¡°ê±´ í™•ì¸
                if "response" in replan_result and replan_result["response"]:
                    # ìµœì¢… ë‹µë³€ ìƒì„±ë¨
                    break
                elif "plan" in replan_result:
                    # ìƒˆë¡œìš´ ê³„íš ìƒì„±ë¨
                    st.markdown("**Updated Plan:**")
                    new_plan = replan_result["plan"]
                    for i, step in enumerate(new_plan, 1):
                        st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                                  unsafe_allow_html=True)
                
                time.sleep(1)
            
            step_num += 1
            
            # ë¬´í•œ ë£¨í”„ ë°©ì§€
            if step_num > max_iterations:
                st.warning("Maximum iteration limit reached.")
                break
        
        # ìµœì¢… ë‹µë³€ í‘œì‹œ
        step_container.empty()  # ì´ì „ ë‹¨ê³„ë“¤ ì •ë¦¬
        
        # ê³¼ì • ìš”ì•½ì„ expanderë¡œ í‘œì‹œ
        with st.expander("ğŸ“Š Process Summary", expanded=False):
            st.markdown("**Original Plan:**")
            for i, step in enumerate(initial_state.get("plan", []), 1):
                st.markdown(f"{i}. {step}")
            
            st.markdown("**Executed Steps:**")
            for i, (step, result) in enumerate(initial_state.get("past_steps", []), 1):
                st.markdown(f"**Step {i}:** {step}")
                st.markdown(f"*Result:* {result[:200]}...")
        
        # ìµœì¢… ë‹µë³€
        st.markdown("### ğŸ’¡ Final Answer")
        final_response = initial_state.get("response", "")
        if not final_response:
            # responseê°€ ì—†ìœ¼ë©´ ìˆ˜ë™ìœ¼ë¡œ ìƒì„±
            final_response = await agent._generate_final_response(initial_state)
        
        st.markdown(f'<div class="result-box">{final_response}</div>', 
                  unsafe_allow_html=True)
        
        return initial_state
        
    except Exception as e:
        st.error(f"Error in complex query processing: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def process_query_with_langgraph_streaming(query: str):
    """LangGraphì˜ ì‹¤í–‰ íë¦„ì„ ì‹¤ì‹œê°„ìœ¼ë¡œ ì¶”ì í•˜ì—¬ í‘œì‹œ"""
    
    # LangGraph ë¹Œë“œ
    graph = build_graph(
        pdf_path=None,
        img_path=None,
        retrieval_type=config.RETRIEVAL_TYPE,
        hybrid_weights=[config.HYBRID_WEIGHT, 1 - config.HYBRID_WEIGHT]
    )
    
    # ì´ˆê¸° ìƒíƒœ
    init_state: GraphState = {"question": [query], "messages": [("user", query)]}
    
    # ì‹¤ì‹œê°„ ì—…ë°ì´íŠ¸ë¥¼ ìœ„í•œ ì»¨í…Œì´ë„ˆë“¤
    main_container = st.empty()
    progress_container = st.empty()
    
    try:
        # LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
        async for event in graph.astream(init_state):
            for node_name, node_output in event.items():
                if node_name == "__end__":
                    continue
                    
                # ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ ì‹¤ì‹œê°„ìœ¼ë¡œ í‘œì‹œ
                with main_container.container():
                    await display_node_execution(node_name, node_output, query)
                    
                # ì ì‹œ ëŒ€ê¸° (ì‚¬ìš©ìê°€ ì½ì„ ìˆ˜ ìˆë„ë¡)
                await asyncio.sleep(1)
        
        # ìµœì¢… ìƒíƒœ ê°€ì ¸ì˜¤ê¸°
        final_state = graph.invoke(init_state)
        
        # ìµœì¢… ê²°ê³¼ í‘œì‹œ
        with main_container.container():
            display_final_results(final_state, query)
            
        return final_state
        
    except Exception as e:
        st.error(f"Error in LangGraph execution: {str(e)}")
        st.error(traceback.format_exc())
        return None


async def display_node_execution(node_name: str, node_output: Dict[str, Any], query: str):
    """ê° ë…¸ë“œì˜ ì‹¤í–‰ ê²°ê³¼ë¥¼ í‘œì‹œ"""
    
    if node_name == "complexity_check":
        # ë³µì¡ë„ ì²´í¬ ê²°ê³¼ í‘œì‹œ
        complexity = node_output.get("next", "unknown")
        st.markdown("### ğŸ” Step 1: Analyzing Question Complexity")
        display_complexity_result(complexity)
        
        if complexity == "simple":
            st.info("ğŸ”„ Proceeding with simple retrieval pipeline...")
        else:
            st.info("ğŸ§  Proceeding with complex Plan-and-Execute pipeline...")
            
    elif node_name in ["retrieve_simple"]:
        # Simple ê²€ìƒ‰ ë‹¨ê³„
        st.markdown("### ğŸ“š Step 2: Information Retrieval")
        with st.spinner("Searching relevant documents..."):
            await asyncio.sleep(0.5)  # ì‹œê°ì  íš¨ê³¼
        
        context = node_output.get("context", [])
        st.success(f"âœ… Retrieved {len(context)} relevant documents")
        
        # ê²€ìƒ‰ëœ ë¬¸ì„œ ë¯¸ë¦¬ë³´ê¸°
        if context:
            with st.expander("ğŸ“– Preview Retrieved Documents", expanded=False):
                for i, doc in enumerate(context[:2], 1):  # ì²˜ìŒ 2ê°œë§Œ
                    content = doc.page_content if hasattr(doc, 'page_content') else str(doc)
                    st.markdown(f"**Document {i}:**")
                    st.markdown(f'<div class="context-box">{content[:300]}...</div>', 
                              unsafe_allow_html=True)
    
    elif node_name == "relevance_check":
        # ê´€ë ¨ì„± ì²´í¬ ë‹¨ê³„
        st.markdown("### ğŸ¯ Step 3: Relevance Filtering")
        with st.spinner("Filtering relevant content..."):
            await asyncio.sleep(0.5)
        
        filtered_context = node_output.get("filtered_context", [])
        scores = node_output.get("scores", [])
        
        if scores:
            avg_score = sum(scores) / len(scores)
            st.success(f"âœ… Filtered content (avg similarity: {avg_score:.3f})")
        else:
            st.success("âœ… Content filtering completed")
    
    elif node_name == "llm_answer_simple":
        # Simple ë‹µë³€ ìƒì„±
        st.markdown("### ğŸ’¡ Step 4: Generating Answer")
        with st.spinner("Generating final answer..."):
            await asyncio.sleep(1)
        
        answer = node_output.get("answer", "")
        if answer:
            st.success("âœ… Answer generated successfully")
        
    elif node_name == "plan_and_execute":
        # Complex ì²˜ë¦¬ ê²°ê³¼
        st.markdown("### ğŸ§  Complex Query Processing Results")
        
        # Plan í‘œì‹œ
        plan = node_output.get("plan", [])
        if plan:
            st.markdown("**ğŸ“‹ Execution Plan:**")
            for i, step in enumerate(plan, 1):
                st.markdown(f'<div class="plan-step">{i}. {step}</div>', 
                          unsafe_allow_html=True)
        
        # ì‹¤í–‰ëœ ë‹¨ê³„ë“¤ í‘œì‹œ
        executed_steps = node_output.get("executed_steps", [])
        if executed_steps:
            with st.expander(f"ğŸ“Š Executed Steps ({len(executed_steps)} steps)", expanded=False):
                for i, (step, result) in enumerate(executed_steps, 1):
                    st.markdown(f"**Step {i}:** {step}")
                    st.markdown(f"*Result:* {result[:200]}...")
                    st.markdown("---")
        
        # ì»¨í…ìŠ¤íŠ¸ í‘œì‹œ
        combined_context = node_output.get("combined_context", "")
        if combined_context:
            with st.expander("ğŸ“š Retrieved Context", expanded=False):
                st.markdown(f'<div class="context-box">{combined_context[:800]}...</div>', 
                          unsafe_allow_html=True)


def display_final_results(final_state: Dict[str, Any], query: str):
    """ìµœì¢… ê²°ê³¼ í‘œì‹œ"""
    st.markdown("---")
    st.markdown("### ğŸ‰ Final Results")
    
    # ìµœì¢… ë‹µë³€
    answer = final_state.get("answer", "No answer generated")
    st.markdown("#### ğŸ’¡ Answer:")
    st.markdown(f'<div class="result-box">{answer}</div>', unsafe_allow_html=True)
    
    # ì²˜ë¦¬ ê²½ë¡œ ìš”ì•½
    explanation = final_state.get("explanation", "")
    if explanation:
        st.markdown("#### ğŸ“ Processing Summary:")
        st.info(explanation)
    
    # í†µê³„ ì •ë³´
    col1, col2, col3 = st.columns(3)
    
    with col1:
        context_count = len(final_state.get("context", []))
        st.metric("ğŸ“š Documents Retrieved", context_count)
    
    with col2:
        # Plan-Execute ì •ë³´
        executed_steps = final_state.get("executed_steps", [])
        if executed_steps:
            st.metric("ğŸ”„ Steps Executed", len(executed_steps))
        else:
            st.metric("ğŸ” Processing Type", "Simple Retrieval")
    
    with col3:
        scores = final_state.get("scores", [])
        if scores and len(scores) > 0:
            avg_score = sum(scores) / len(scores)
            st.metric("ğŸ¯ Avg Similarity", f"{avg_score:.3f}")


def main():
    """ë©”ì¸ Streamlit ì•±"""
    initialize_session_state()
    
    # í—¤ë”
    st.markdown('<div class="main-header">ğŸ”¬ Semiconductor Physics RAG System</div>', 
              unsafe_allow_html=True)
    
    # ì‚¬ì´ë“œë°” ì„¤ì •
    with st.sidebar:
        st.markdown("### âš™ï¸ Configuration")
        st.markdown(f"**Model:** {config.OPENAI_MODEL}")
        st.markdown(f"**Retrieval Type:** {config.RETRIEVAL_TYPE}")
        st.markdown(f"**TOP_K:** {config.TOP_K}")
        st.markdown(f"**Max Plan Steps:** {config.MAX_PLAN_STEPS}")
        
        st.markdown("### ğŸ“Š Graph Flow")
        st.markdown("""
        **LangGraph Execution:**
        1. ğŸ” Complexity Check
        2a. ğŸ“š Simple: Retrieve â†’ Filter â†’ Answer
        2b. ğŸ§  Complex: Plan-and-Execute
        3. ğŸ’¡ Final Answer
        """)
        
        if st.button("ğŸ”„ Reset Session"):
            for key in st.session_state.keys():
                del st.session_state[key]
            st.rerun()
    
    # ë©”ì¸ ì±„íŒ… ì¸í„°í˜ì´ìŠ¤
    st.markdown("### ğŸ’¬ Ask a Question")
    
    # ì±„íŒ… ë©”ì‹œì§€ í‘œì‹œ
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])
    
    # ì‚¬ìš©ì ì…ë ¥
    if prompt := st.chat_input("Enter your semiconductor physics question..."):
        # ì‚¬ìš©ì ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ì‘ë‹µ ì‹œì‘
        with st.chat_message("assistant"):
            st.markdown("### ğŸš€ Processing Your Question")
            st.markdown(f"**Query:** {prompt}")
            
            # LangGraph ìŠ¤íŠ¸ë¦¬ë° ì‹¤í–‰
            try:
                # ë¹„ë™ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ ë˜í¼
                try:
                    current_loop = asyncio.get_running_loop()
                    # ì´ë¯¸ ë£¨í”„ê°€ ì‹¤í–‰ ì¤‘ì´ë©´ ìƒˆ ìŠ¤ë ˆë“œì—ì„œ ì‹¤í–‰
                    import concurrent.futures
                    
                    def run_async_in_thread():
                        new_loop = asyncio.new_event_loop()
                        asyncio.set_event_loop(new_loop)
                        try:
                            return new_loop.run_until_complete(
                                process_query_with_langgraph_streaming(prompt)
                            )
                        finally:
                            new_loop.close()
                    
                    with concurrent.futures.ThreadPoolExecutor() as executor:
                        future = executor.submit(run_async_in_thread)
                        result = future.result()
                        
                except RuntimeError:
                    # ì‹¤í–‰ ì¤‘ì¸ ë£¨í”„ê°€ ì—†ìœ¼ë©´ ì§ì ‘ ì‹¤í–‰
                    result = asyncio.run(process_query_with_langgraph_streaming(prompt))
                
                if result:
                    answer = result.get("answer", "No answer generated")
                    complexity = "Complex" if result.get("executed_steps") else "Simple"
                    response_content = f"**Question Complexity:** {complexity}\n\n**Answer:** {answer}"
                else:
                    response_content = "**Error:** Failed to process question"
                    
            except Exception as e:
                st.error(f"Error processing question: {str(e)}")
                response_content = f"**Error:** {str(e)}"
        
        # ì–´ì‹œìŠ¤í„´íŠ¸ ë©”ì‹œì§€ ì¶”ê°€
        st.session_state.messages.append({"role": "assistant", "content": response_content})


if __name__ == "__main__":
    main()
